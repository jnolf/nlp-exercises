{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fc8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import acquire as a\n",
    "from time import strftime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac3b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title     published  \\\n",
      "0              Learn to Code: Python Workshop on 4/23  Mar 31, 2022   \n",
      "1                   Coming Soon: Cloud Administration  Mar 17, 2022   \n",
      "2             5 Books Every Woman In Tech Should Read   Mar 8, 2022   \n",
      "3                   Codeup Start Dates for March 2022  Jan 26, 2022   \n",
      "4   VET TEC Funding Now Available For Dallas Veterans   Jan 7, 2022   \n",
      "5       Dallas Campus Re-opens With New Grant Partner  Dec 30, 2021   \n",
      "6   Codeup’s Placement Team Continues Setting Records  Nov 19, 2021   \n",
      "7   IT Certifications 101: Why They Matter, and Wh...  Nov 18, 2021   \n",
      "8   A rise in cyber attacks means opportunities fo...  Nov 17, 2021   \n",
      "9    Use your GI Bill® benefits to Land a Job in Tech   Nov 4, 2021   \n",
      "10  Which program is right for me: Cyber Security ...  Oct 28, 2021   \n",
      "11               What the Heck is System Engineering?  Oct 21, 2021   \n",
      "12     From Speech Pathology to Business Intelligence  Oct 18, 2021   \n",
      "13                      Boris – Behind the Billboards   Oct 3, 2021   \n",
      "14  Is Codeup the Best Bootcamp in San Antonio…or ...  Sep 16, 2021   \n",
      "\n",
      "                                              content  \n",
      "0   According to LinkedIn, the “#1 Most Promising ...  \n",
      "1   We’re launching a new program out of San Anton...  \n",
      "2   On this International Women’s Day 2022 we want...  \n",
      "3   As we approach the end of January we wanted to...  \n",
      "4   We are so happy to announce that VET TEC benef...  \n",
      "5   We are happy to announce that our Dallas campu...  \n",
      "6   Our Placement Team is simply defined as a grou...  \n",
      "7   AWS, Google, Azure, Red Hat, CompTIA…these are...  \n",
      "8   In the last few months, the US has experienced...  \n",
      "9   As the end of military service gets closer, ma...  \n",
      "10  What IT Career should I choose?\\nIf you’re thi...  \n",
      "11  Codeup offers a 13-week training program: Syst...  \n",
      "12  By: Alicia Gonzalez\\nBefore Codeup, I was a ho...  \n",
      "13                                                     \n",
      "14  Looking for the best data science bootcamp in ...  \n"
     ]
    }
   ],
   "source": [
    "original = a.get_codeup_articles()\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b33d2b",
   "metadata": {},
   "source": [
    "#### 1.)Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f6e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    Function takes in a string and returns the same string normalized\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .mdecode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a8967",
   "metadata": {},
   "source": [
    "#### 2.) Define a function named tokenize. \n",
    "It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc05a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e00359",
   "metadata": {},
   "source": [
    "#### 3.) Define a function named stem. \n",
    "It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5635e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273b556",
   "metadata": {},
   "source": [
    "#### 4.) Define a function named lemmatize. \n",
    "It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8366810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5670a0",
   "metadata": {},
   "source": [
    "#### 5.) Define a function named remove_stopwords. \n",
    "It should accept some text and return the text after removing all the stopwords. This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f0316",
   "metadata": {},
   "source": [
    "#### 6.) Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d909ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting articles for business\n",
      "Getting articles for sports\n",
      "Getting articles for entertainment\n",
      "Getting articles for technology\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>India's retail inflation surges to 7.79% in Ap...</td>\n",
       "      <td>India's retail inflation surged to 7.79% in Ap...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>2022-05-12T12:41:14.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Rupee hits new all-time low, slips to 77.55 ag...</td>\n",
       "      <td>The Indian rupee has touched a fresh all-time ...</td>\n",
       "      <td>Apaar Sharma</td>\n",
       "      <td>2022-05-12T05:58:52.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>List of 10 highest-paid sportspersons released...</td>\n",
       "      <td>Argentina and PSG forward Lionel Messi was the...</td>\n",
       "      <td>Anmol Sharma</td>\n",
       "      <td>2022-05-12T05:22:54.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Saudi Aramco dethrones Apple as world's most v...</td>\n",
       "      <td>World's biggest crude exporter Saudi Aramco ha...</td>\n",
       "      <td>Anmol Sharma</td>\n",
       "      <td>2022-05-12T05:40:33.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Over $200 billion wiped off cryptocurrency mar...</td>\n",
       "      <td>More than $200 billion of wealth was wiped out...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>2022-05-12T12:19:46.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0  business  India's retail inflation surges to 7.79% in Ap...   \n",
       "1  business  Rupee hits new all-time low, slips to 77.55 ag...   \n",
       "2  business  List of 10 highest-paid sportspersons released...   \n",
       "3  business  Saudi Aramco dethrones Apple as world's most v...   \n",
       "4  business  Over $200 billion wiped off cryptocurrency mar...   \n",
       "\n",
       "                                             content          author  \\\n",
       "0  India's retail inflation surged to 7.79% in Ap...  Pragya Swastik   \n",
       "1  The Indian rupee has touched a fresh all-time ...    Apaar Sharma   \n",
       "2  Argentina and PSG forward Lionel Messi was the...    Anmol Sharma   \n",
       "3  World's biggest crude exporter Saudi Aramco ha...    Anmol Sharma   \n",
       "4  More than $200 billion of wealth was wiped out...  Pragya Swastik   \n",
       "\n",
       "                  published  \n",
       "0  2022-05-12T12:41:14.000Z  \n",
       "1  2022-05-12T05:58:52.000Z  \n",
       "2  2022-05-12T05:22:54.000Z  \n",
       "3  2022-05-12T05:40:33.000Z  \n",
       "4  2022-05-12T12:19:46.000Z  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = a.get_inshorts_articles()\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efc042",
   "metadata": {},
   "source": [
    "#### 7.) Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7046c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learn to Code: Python Workshop on 4/23</td>\n",
       "      <td>Mar 31, 2022</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coming Soon: Cloud Administration</td>\n",
       "      <td>Mar 17, 2022</td>\n",
       "      <td>We’re launching a new program out of San Anton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 Books Every Woman In Tech Should Read</td>\n",
       "      <td>Mar 8, 2022</td>\n",
       "      <td>On this International Women’s Day 2022 we want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Codeup Start Dates for March 2022</td>\n",
       "      <td>Jan 26, 2022</td>\n",
       "      <td>As we approach the end of January we wanted to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VET TEC Funding Now Available For Dallas Veterans</td>\n",
       "      <td>Jan 7, 2022</td>\n",
       "      <td>We are so happy to announce that VET TEC benef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     published  \\\n",
       "0             Learn to Code: Python Workshop on 4/23  Mar 31, 2022   \n",
       "1                  Coming Soon: Cloud Administration  Mar 17, 2022   \n",
       "2            5 Books Every Woman In Tech Should Read   Mar 8, 2022   \n",
       "3                  Codeup Start Dates for March 2022  Jan 26, 2022   \n",
       "4  VET TEC Funding Now Available For Dallas Veterans   Jan 7, 2022   \n",
       "\n",
       "                                             content  \n",
       "0  According to LinkedIn, the “#1 Most Promising ...  \n",
       "1  We’re launching a new program out of San Anton...  \n",
       "2  On this International Women’s Day 2022 we want...  \n",
       "3  As we approach the end of January we wanted to...  \n",
       "4  We are so happy to announce that VET TEC benef...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = a.get_codeup_articles()\n",
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4e218",
   "metadata": {},
   "source": [
    "#### 8.) For each dataframe, produce the following columns:\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617efc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'content': 'original'}, inplace=True)\n",
    "codeup_df.rename(columns={'content': 'original'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5db63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090130c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indias retail inflation surges to 779 in april highest in 8 years'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_df = basic_clean(news_df.title[0])\n",
    "ex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8989d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d29799fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indias retail inflation surges to 779 in april highest in 8 years'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_df = tokenize(ex_df)\n",
    "tok_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad3f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a12e9062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'india retail inflat surg to 779 in april highest in 8 year'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_df = stem(tok_df)\n",
    "stem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9647713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6beb3494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'india retail inflat surg to 779 in april highest in 8 year'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_df = lemmatize(stem_df)\n",
    "lem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2400d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dfddeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998131d",
   "metadata": {},
   "source": [
    "news_df prepped..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61a76ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India's retail inflation surges to 7.79% in Ap...</td>\n",
       "      <td>India's retail inflation surged to 7.79% in Ap...</td>\n",
       "      <td>indias retail inflation surged 779 april 2022 ...</td>\n",
       "      <td>india retail inflat surg 779 april 2022 highes...</td>\n",
       "      <td>india retail inflation surged 779 april 2022 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rupee hits new all-time low, slips to 77.55 ag...</td>\n",
       "      <td>The Indian rupee has touched a fresh all-time ...</td>\n",
       "      <td>indian rupee touched fresh alltime low 7755 us...</td>\n",
       "      <td>indian rupe touch fresh alltim low 7755 us dol...</td>\n",
       "      <td>indian rupee touched fresh alltime low 7755 u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>List of 10 highest-paid sportspersons released...</td>\n",
       "      <td>Argentina and PSG forward Lionel Messi was the...</td>\n",
       "      <td>argentina psg forward lionel messi highestpaid...</td>\n",
       "      <td>argentina psg forward lionel messi wa highestp...</td>\n",
       "      <td>argentina psg forward lionel messi wa highestp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saudi Aramco dethrones Apple as world's most v...</td>\n",
       "      <td>World's biggest crude exporter Saudi Aramco ha...</td>\n",
       "      <td>worlds biggest crude exporter saudi aramco ove...</td>\n",
       "      <td>world biggest crude export saudi aramco overta...</td>\n",
       "      <td>world biggest crude exporter saudi aramco over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over $200 billion wiped off cryptocurrency mar...</td>\n",
       "      <td>More than $200 billion of wealth was wiped out...</td>\n",
       "      <td>200 billion wealth wiped cryptocurrency market...</td>\n",
       "      <td>200 billion wealth wa wipe cryptocurr market 2...</td>\n",
       "      <td>200 billion wealth wa wiped cryptocurrency mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  India's retail inflation surges to 7.79% in Ap...   \n",
       "1  Rupee hits new all-time low, slips to 77.55 ag...   \n",
       "2  List of 10 highest-paid sportspersons released...   \n",
       "3  Saudi Aramco dethrones Apple as world's most v...   \n",
       "4  Over $200 billion wiped off cryptocurrency mar...   \n",
       "\n",
       "                                            original  \\\n",
       "0  India's retail inflation surged to 7.79% in Ap...   \n",
       "1  The Indian rupee has touched a fresh all-time ...   \n",
       "2  Argentina and PSG forward Lionel Messi was the...   \n",
       "3  World's biggest crude exporter Saudi Aramco ha...   \n",
       "4  More than $200 billion of wealth was wiped out...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  indias retail inflation surged 779 april 2022 ...   \n",
       "1  indian rupee touched fresh alltime low 7755 us...   \n",
       "2  argentina psg forward lionel messi highestpaid...   \n",
       "3  worlds biggest crude exporter saudi aramco ove...   \n",
       "4  200 billion wealth wiped cryptocurrency market...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  india retail inflat surg 779 april 2022 highes...   \n",
       "1  indian rupe touch fresh alltim low 7755 us dol...   \n",
       "2  argentina psg forward lionel messi wa highestp...   \n",
       "3  world biggest crude export saudi aramco overta...   \n",
       "4  200 billion wealth wa wipe cryptocurr market 2...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  india retail inflation surged 779 april 2022 h...  \n",
       "1  indian rupee touched fresh alltime low 7755 u ...  \n",
       "2  argentina psg forward lionel messi wa highestp...  \n",
       "3  world biggest crude exporter saudi aramco over...  \n",
       "4  200 billion wealth wa wiped cryptocurrency mar...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prep_article_data(news_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50b44a",
   "metadata": {},
   "source": [
    "code_up_df prepped..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eda0d8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learn to Code: Python Workshop on 4/23</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "      <td>according linkedin 1 promising job data scienc...</td>\n",
       "      <td>accord linkedin 1 promis job data scienc codeu...</td>\n",
       "      <td>according linkedin 1 promising job data scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coming Soon: Cloud Administration</td>\n",
       "      <td>We’re launching a new program out of San Anton...</td>\n",
       "      <td>launching new program san antonio acquisition ...</td>\n",
       "      <td>launch new program san antonio acquisit racksp...</td>\n",
       "      <td>launching new program san antonio acquisition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 Books Every Woman In Tech Should Read</td>\n",
       "      <td>On this International Women’s Day 2022 we want...</td>\n",
       "      <td>international womens day 2022 wanted tell stor...</td>\n",
       "      <td>thi intern women day 2022 want tell stori wome...</td>\n",
       "      <td>international woman day 2022 wanted tell story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Codeup Start Dates for March 2022</td>\n",
       "      <td>As we approach the end of January we wanted to...</td>\n",
       "      <td>approach end january wanted look forward next ...</td>\n",
       "      <td>approach end januari want look forward next st...</td>\n",
       "      <td>approach end january wanted look forward next ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VET TEC Funding Now Available For Dallas Veterans</td>\n",
       "      <td>We are so happy to announce that VET TEC benef...</td>\n",
       "      <td>happy announce vet tec benefits available used...</td>\n",
       "      <td>happi announc vet tec benefit avail use campu ...</td>\n",
       "      <td>happy announce vet tec benefit available used ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0             Learn to Code: Python Workshop on 4/23   \n",
       "1                  Coming Soon: Cloud Administration   \n",
       "2            5 Books Every Woman In Tech Should Read   \n",
       "3                  Codeup Start Dates for March 2022   \n",
       "4  VET TEC Funding Now Available For Dallas Veterans   \n",
       "\n",
       "                                            original  \\\n",
       "0  According to LinkedIn, the “#1 Most Promising ...   \n",
       "1  We’re launching a new program out of San Anton...   \n",
       "2  On this International Women’s Day 2022 we want...   \n",
       "3  As we approach the end of January we wanted to...   \n",
       "4  We are so happy to announce that VET TEC benef...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  according linkedin 1 promising job data scienc...   \n",
       "1  launching new program san antonio acquisition ...   \n",
       "2  international womens day 2022 wanted tell stor...   \n",
       "3  approach end january wanted look forward next ...   \n",
       "4  happy announce vet tec benefits available used...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  accord linkedin 1 promis job data scienc codeu...   \n",
       "1  launch new program san antonio acquisit racksp...   \n",
       "2  thi intern women day 2022 want tell stori wome...   \n",
       "3  approach end januari want look forward next st...   \n",
       "4  happi announc vet tec benefit avail use campu ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  according linkedin 1 promising job data scienc...  \n",
       "1  launching new program san antonio acquisition ...  \n",
       "2  international woman day 2022 wanted tell story...  \n",
       "3  approach end january wanted look forward next ...  \n",
       "4  happy announce vet tec benefit available used ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_article_data(codeup_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c000e76",
   "metadata": {},
   "source": [
    "#### Ask yourself:\n",
    "\n",
    "If your corpus is 493KB, would you prefer to use stemmed or lemmatized text? \n",
    "##### Lemmatizing\n",
    "\n",
    "If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "##### Lemmatizing or Stemming\n",
    "\n",
    "If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text? \n",
    "##### Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be8a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
